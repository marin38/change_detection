{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import shutil\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_int_zeros(i, num_digits):\n",
    "    res = str(i)\n",
    "    for j in range(num_digits-len(res)):\n",
    "        res = '0'+res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_VAL_TEST_DIR='data/train_test_val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataPreparator:\n",
    "    def __init__(self, images_1, images_2, cdmaps, dimx, dimy, invert_gt):\n",
    "        self.images_1 = images_1\n",
    "        self.images_2 = images_2\n",
    "        self.cdmaps = cdmaps\n",
    "        self.dimx = dimx\n",
    "        self.dimy = dimy\n",
    "        self.train_and_val = []\n",
    "        self.test = []\n",
    "        self.invert_gt=invert_gt\n",
    "        \n",
    "    def remove_and_create_dirs(self):\n",
    "        try:\n",
    "            shutil.rmtree(TRAIN_VAL_TEST_DIR)\n",
    "        finally:\n",
    "            os.mkdir(TRAIN_VAL_TEST_DIR)\n",
    "            dir_names=['1/', '2/', 'gt/']\n",
    "            os.mkdir(TRAIN_VAL_TEST_DIR+'train/')\n",
    "            os.mkdir(TRAIN_VAL_TEST_DIR+'val/')\n",
    "            os.mkdir(TRAIN_VAL_TEST_DIR+'test/')\n",
    "            \n",
    "            for dir_name in dir_names:\n",
    "                os.mkdir(TRAIN_VAL_TEST_DIR+'train/'+dir_name)\n",
    "                os.mkdir(TRAIN_VAL_TEST_DIR+'val/'+dir_name)\n",
    "                os.mkdir(TRAIN_VAL_TEST_DIR+'test/'+dir_name)\n",
    "                \n",
    "    def augment(self, aug, new_img_1, new_img_2, new_img_gt):\n",
    "        if aug[0]:\n",
    "            if random.random()>0.5:\n",
    "                new_img_gt = new_img_gt[::-1, ::]\n",
    "                new_img_1 = new_img_1[::-1, ::]\n",
    "                new_img_2 = new_img_2[::-1, ::]\n",
    "        if aug[1]:\n",
    "            if random.random()>0.5:\n",
    "                new_img_gt = new_img_gt[::, ::-1]\n",
    "                new_img_1 = new_img_1[::, ::-1]\n",
    "                new_img_2 = new_img_2[::, ::-1]\n",
    "        if aug[2]:\n",
    "            if random.random()>0.5:\n",
    "                rows,cols = new_img_gt.shape\n",
    "                M = cv2.getRotationMatrix2D((cols/2,rows/2),180,1)\n",
    "                new_img_gt = cv2.warpAffine(new_img_gt,M,(cols,rows))\n",
    "                new_img_1 = cv2.warpAffine(new_img_1,M,(cols,rows))\n",
    "                new_img_2 = cv2.warpAffine(new_img_2,M,(cols,rows))\n",
    "        return new_img_1, new_img_2, new_img_gt\n",
    "\n",
    "            \n",
    "            \n",
    "    def create_patches(self, dir_name, dataset_indicies, num_samples=10, aug=[True, True, True]):\n",
    "        img_pos_index=0\n",
    "        img_neg_index=0\n",
    "        for i in tqdm(range(len(dataset_indicies))):\n",
    "            for j in (range(num_samples)):\n",
    "                # POSITIVES\n",
    "                img_1 = cv2.imread(self.images_1[dataset_indicies[i]])\n",
    "                img_2 = cv2.imread(self.images_2[dataset_indicies[i]])\n",
    "                img_gt = cv2.imread(self.cdmaps[dataset_indicies[i]],0)\n",
    "                \n",
    "                if self.invert_gt:\n",
    "                    img_gt = cv2.bitwise_not(img_gt)\n",
    "                \n",
    "                ret,thresh = cv2.threshold(img_gt,127,255,0)\n",
    "                im2, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "                for counter in contours:\n",
    "                    M = cv2.moments(counter)\n",
    "                    #print (counter)\n",
    "                    if M['m00'] == 0:\n",
    "                        continue\n",
    "                    cx = int(M['m10']/M['m00'])\n",
    "                    cy = int(M['m01']/M['m00'])\n",
    "                    \n",
    "                    cx = cx + (random.random()*2-1)*0.15*img_gt.shape[1]\n",
    "                    cy = cy + (random.random()*2-1)*0.15*img_gt.shape[0]\n",
    "                    \n",
    "                    cx = int(np.clip(cx, self.dimx//2, img_gt.shape[1]-self.dimx//2))\n",
    "                    cy = int(np.clip(cy, self.dimy//2, img_gt.shape[0]-self.dimy//2))\n",
    "\n",
    "                    new_img_gt = img_gt[cy-self.dimy//2:cy+self.dimy//2, cx-self.dimx//2:cx+self.dimx//2]\n",
    "                    new_img_1 = img_1[cy-self.dimy//2:cy+self.dimy//2, cx-self.dimx//2:cx+self.dimx//2]\n",
    "                    new_img_2 = img_2[cy-self.dimy//2:cy+self.dimy//2, cx-self.dimx//2:cx+self.dimx//2]\n",
    "                    \n",
    "                    new_img_1, new_img_2, new_img_gt = self.augment(aug, new_img_1, new_img_2, new_img_gt)\n",
    "\n",
    "                    cv2.imwrite(dir_name+'gt/'+'P'+pad_int_zeros(img_pos_index, 6)+'.png', new_img_gt)\n",
    "                    cv2.imwrite(dir_name+'1/'+'P'+pad_int_zeros(img_pos_index, 6)+'.png', new_img_1)\n",
    "                    cv2.imwrite(dir_name+'2/'+'P'+pad_int_zeros(img_pos_index, 6)+'.png', new_img_2)\n",
    "                    img_pos_index += 1\n",
    "                    \n",
    "                # NEGATIVES\n",
    "                for counter in contours:\n",
    "                    cx = random.randint(self.dimx//2, img_gt.shape[1]-self.dimx//2)\n",
    "                    cy = random.randint(self.dimy//2, img_gt.shape[0]-self.dimy//2)\n",
    "                    \n",
    "                    new_img_gt=img_gt[cy-self.dimy//2:cy+self.dimy//2, cx-self.dimx//2:cx+self.dimx//2]\n",
    "                    new_img_1=img_1[cy-self.dimy//2:cy+self.dimy//2, cx-self.dimx//2:cx+self.dimx//2]\n",
    "                    new_img_2=img_2[cy-self.dimy//2:cy+self.dimy//2, cx-self.dimx//2:cx+self.dimx//2]\n",
    "                    \n",
    "                    new_img_1, new_img_2, new_img_gt = self.augment(aug, new_img_1, new_img_2, new_img_gt)\n",
    "\n",
    "                    cv2.imwrite(dir_name+'gt/'+'N'+pad_int_zeros(img_neg_index, 6)+'.png', new_img_gt)\n",
    "                    cv2.imwrite(dir_name+'1/'+'N'+pad_int_zeros(img_neg_index, 6)+'.png', new_img_1)\n",
    "                    cv2.imwrite(dir_name+'2/'+'N'+pad_int_zeros(img_neg_index, 6)+'.png', new_img_2)\n",
    "                    img_neg_index += 1\n",
    "            \n",
    "                \n",
    "        \n",
    "    def generate_train_val_test_datasets(self, test_rate=0.01, val_rate=0.2):\n",
    "        assert len(self.images_1) == len(self.images_2)\n",
    "        assert len(self.images_1) == len(self.cdmaps)\n",
    "        len_dataset = len(self.images_1)\n",
    "        len_test_set = int(test_rate*len_dataset)\n",
    "        test_indicies = random.sample(range(len(self.images_1)), len_test_set)\n",
    "        train_val_indicies = []\n",
    "        \n",
    "        for i in range(len_dataset):\n",
    "            if i not in test_indicies:\n",
    "                train_val_indicies.append(i)\n",
    "        \n",
    "        self.remove_and_create_dirs()\n",
    "        \n",
    "        self.create_patches(TRAIN_VAL_TEST_DIR+'test/', test_indicies)\n",
    "        self.create_patches(TRAIN_VAL_TEST_DIR+'train/', train_val_indicies)\n",
    "        \n",
    "        train_val_patches = os.listdir(TRAIN_VAL_TEST_DIR+'train/1/')\n",
    "        val_indicies = random.sample(range(len(train_val_patches)), int(val_rate*len(train_val_patches)))\n",
    "        val_patches = []\n",
    "        for i in val_indicies:\n",
    "            val_patches.append(train_val_patches[i])\n",
    "        \n",
    "        for patch in val_patches:\n",
    "            os.rename(TRAIN_VAL_TEST_DIR+'train/1/'+patch, TRAIN_VAL_TEST_DIR+'val/1/'+patch)\n",
    "            os.rename(TRAIN_VAL_TEST_DIR+'train/2/'+patch, TRAIN_VAL_TEST_DIR+'val/2/'+patch)\n",
    "            os.rename(TRAIN_VAL_TEST_DIR+'train/gt/'+patch, TRAIN_VAL_TEST_DIR+'val/gt/'+patch)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#AICD Dataset preparation\n",
    "\n",
    "DATASET_DIR = 'data/AICDDataset/'\n",
    "images_1 = []\n",
    "images_2 = []\n",
    "gt_array = []\n",
    "\n",
    "for i_scene in range(100):\n",
    "    for i_view in range(5):\n",
    "        img_dir = DATASET_DIR+'Images_NoShadow/'\n",
    "        gt_dir = DATASET_DIR+'GroundTruth/'\n",
    "        base_name = 'Scene'+pad_int_zeros(i_scene, 4)+'_View'+pad_int_zeros(i_view, 2)\n",
    "        \n",
    "        img_1 = img_dir+base_name+'_moving.png'\n",
    "        img_2 = img_dir+base_name+'_target.png'\n",
    "        gt = gt_dir+base_name+'_gtmask.png'\n",
    "        if not((os.path.isfile(img_1)) and (os.path.isfile(img_2)) and (os.path.isfile(gt))):\n",
    "            print(base_name+' not exists')\n",
    "        else:\n",
    "            images_1.append(img_1)\n",
    "            images_2.append(img_2)\n",
    "            gt_array.append(gt)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TSUNAMI Dataset preparation\n",
    "\n",
    "DATASET_DIR = 'data/TSUNAMI/'\n",
    "images_1 = []\n",
    "images_2 = []\n",
    "gt_array = []\n",
    "\n",
    "for i_image in range(100):\n",
    "    base_name = pad_int_zeros(i_image, 8)\n",
    "\n",
    "    img_1 = DATASET_DIR+'t0/'+base_name+'.jpg'\n",
    "    img_2 = DATASET_DIR+'t1/'+base_name+'.jpg'\n",
    "    gt = DATASET_DIR+'ground_truth/'+base_name+'.bmp'\n",
    "    if not((os.path.isfile(img_1)) and (os.path.isfile(img_2)) and (os.path.isfile(gt))):\n",
    "        print(img_1, img_2, gt, ' not exists')\n",
    "    else:\n",
    "        images_1.append(img_1)\n",
    "        images_2.append(img_2)\n",
    "        gt_array.append(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/envs/tensorflow/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/user/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/user/anaconda3/envs/tensorflow/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.42s/it]\n",
      "100%|██████████| 99/99 [08:38<00:00,  5.24s/it]\n"
     ]
    }
   ],
   "source": [
    "dp = DataPreparator(images_1, images_2, gt_array, 240, 192, invert_gt=True)\n",
    "dp.remove_and_create_dirs()\n",
    "dp.generate_train_val_test_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3250"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('data/train_test_val_tsunami_reduced/'+'val/gt/'))//5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
