{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import shutil\n",
    "from utils import pad_int_zeros\n",
    "%matplotlib inline\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_VAL_TEST_DIR='../data/aicd_patches/fixed_test_newAug_testR=0.1_valT=0.2_numSamples=30/'\n",
    "DATASET='AICD' # AICD / TSUNAMI\n",
    "DATASET_DIR = '../data/DATASETS/AICDDataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataPreparator:\n",
    "    def __init__(self, images_1, images_2, cdmaps, dimx, dimy, invert_gt):\n",
    "        self.images_1 = images_1\n",
    "        self.images_2 = images_2\n",
    "        self.cdmaps = cdmaps\n",
    "        self.dimx = dimx\n",
    "        self.dimy = dimy\n",
    "        self.train_and_val = []\n",
    "        self.test = []\n",
    "        self.invert_gt=invert_gt\n",
    "        \n",
    "    def remove_and_create_dirs(self):\n",
    "        try:\n",
    "            shutil.rmtree(TRAIN_VAL_TEST_DIR)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        finally:\n",
    "            os.mkdir(TRAIN_VAL_TEST_DIR)\n",
    "            dir_names=['1/', '2/', 'gt/']\n",
    "            os.mkdir(TRAIN_VAL_TEST_DIR+'train/')\n",
    "            os.mkdir(TRAIN_VAL_TEST_DIR+'val/')\n",
    "            os.mkdir(TRAIN_VAL_TEST_DIR+'test/')\n",
    "            \n",
    "            for dir_name in dir_names:\n",
    "                os.mkdir(TRAIN_VAL_TEST_DIR+'train/'+dir_name)\n",
    "                os.mkdir(TRAIN_VAL_TEST_DIR+'val/'+dir_name)\n",
    "                os.mkdir(TRAIN_VAL_TEST_DIR+'test/'+dir_name)\n",
    "                \n",
    "    def augment(self, img_1, img_2, img_gt):\n",
    "        assert img_1.shape == img_2.shape\n",
    "        assert img_1.shape[:2] == img_gt.shape[0:2]\n",
    "        \n",
    "        sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "        \n",
    "        # sequence for images and gt\n",
    "        seq_all = iaa.Sequential([\n",
    "            iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "            iaa.Flipud(0.5), # vertically flip 50% of all images\n",
    "            \n",
    "            sometimes(iaa.Affine(\n",
    "                scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n",
    "                translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n",
    "                rotate=(-45, 45), # rotate by -45 to +45 degrees\n",
    "                shear=(-16, 16), # shear by -16 to +16 degrees\n",
    "                order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "                mode='reflect' # use reflections\n",
    "            ))], \n",
    "            random_order=True)\n",
    "        seq_all_det = seq_all.to_deterministic()\n",
    "        \n",
    "        # augment images and gt\n",
    "        new_img_1 = seq_all_det.augment_images([img_1])[0]\n",
    "        new_img_2 = seq_all_det.augment_images([img_2])[0]\n",
    "        new_img_gt = seq_all_det.augment_images([img_gt])[0]\n",
    "        \n",
    "        # add some non-deterministic salt-n-papper for images\n",
    "        seq_images = iaa.Sequential([iaa.SaltAndPepper(0.03)])\n",
    "        new_img_1 = seq_images.augment_images([new_img_1])[0]\n",
    "        new_img_2 = seq_images.augment_images([new_img_2])[0]\n",
    "        \n",
    "        return new_img_1, new_img_2, new_img_gt\n",
    "\n",
    "            \n",
    "            \n",
    "    def create_patches(self, dir_name, dataset_indicies, translate_rate=0.10, num_samples=30):\n",
    "        img_pos_index=0\n",
    "        img_neg_index=0\n",
    "        for i in tqdm(range(len(dataset_indicies))):\n",
    "            for j in (range(num_samples)):\n",
    "                img_1 = cv2.imread(self.images_1[dataset_indicies[i]])\n",
    "                img_2 = cv2.imread(self.images_2[dataset_indicies[i]])\n",
    "                img_gt = cv2.imread(self.cdmaps[dataset_indicies[i]],0)\n",
    "                if self.invert_gt:\n",
    "                    img_gt = cv2.bitwise_not(img_gt)\n",
    "                ret,thresh = cv2.threshold(img_gt,127,255,0)\n",
    "                im2, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "                \n",
    "                # POSITIVE CLASS ~ CHANGES\n",
    "                for counter in contours:\n",
    "                    M = cv2.moments(counter)\n",
    "                    #print (counter)\n",
    "                    if M['m00'] == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    # find center of change\n",
    "                    cx = int(M['m10']/M['m00'])\n",
    "                    cy = int(M['m01']/M['m00'])\n",
    "                    \n",
    "                    # translate randomly to translate_rate                   \n",
    "                    cx = cx + (random.random()*2-1)*translate_rate*img_gt.shape[1]\n",
    "                    cy = cy + (random.random()*2-1)*translate_rate*img_gt.shape[0]\n",
    "                    \n",
    "                    cx = int(np.clip(cx, self.dimx//2, img_gt.shape[1]-self.dimx//2))\n",
    "                    cy = int(np.clip(cy, self.dimy//2, img_gt.shape[0]-self.dimy//2))\n",
    "\n",
    "                    \n",
    "                    new_img_gt = img_gt[cy-self.dimy//2:cy+self.dimy//2, cx-self.dimx//2:cx+self.dimx//2]\n",
    "                    new_img_1 = img_1[cy-self.dimy//2:cy+self.dimy//2, cx-self.dimx//2:cx+self.dimx//2]\n",
    "                    new_img_2 = img_2[cy-self.dimy//2:cy+self.dimy//2, cx-self.dimx//2:cx+self.dimx//2]\n",
    "                    \n",
    "                    # augment one patch\n",
    "                    new_img_1, new_img_2, new_img_gt = self.augment(new_img_1, new_img_2, new_img_gt)\n",
    "\n",
    "                    cv2.imwrite(dir_name+'gt/'+'P'+pad_int_zeros(img_pos_index, 6)+'.png', new_img_gt)\n",
    "                    cv2.imwrite(dir_name+'1/'+'P'+pad_int_zeros(img_pos_index, 6)+'.png', new_img_1)\n",
    "                    cv2.imwrite(dir_name+'2/'+'P'+pad_int_zeros(img_pos_index, 6)+'.png', new_img_2)\n",
    "                    img_pos_index += 1\n",
    "                    \n",
    "                # NEGATIVE CLASS ~ NO CHANGES\n",
    "                for counter in contours:\n",
    "                    # take random patch\n",
    "                    cx = random.randint(self.dimx//2, img_gt.shape[1]-self.dimx//2)\n",
    "                    cy = random.randint(self.dimy//2, img_gt.shape[0]-self.dimy//2)\n",
    "                    \n",
    "                    new_img_gt=img_gt[cy-self.dimy//2:cy+self.dimy//2, cx-self.dimx//2:cx+self.dimx//2]\n",
    "                    new_img_1=img_1[cy-self.dimy//2:cy+self.dimy//2, cx-self.dimx//2:cx+self.dimx//2]\n",
    "                    new_img_2=img_2[cy-self.dimy//2:cy+self.dimy//2, cx-self.dimx//2:cx+self.dimx//2]\n",
    "                    \n",
    "                    # augment one patch\n",
    "                    new_img_1, new_img_2, new_img_gt = self.augment(new_img_1, new_img_2, new_img_gt)\n",
    "\n",
    "                    cv2.imwrite(dir_name+'gt/'+'N'+pad_int_zeros(img_neg_index, 6)+'.png', new_img_gt)\n",
    "                    cv2.imwrite(dir_name+'1/'+'N'+pad_int_zeros(img_neg_index, 6)+'.png', new_img_1)\n",
    "                    cv2.imwrite(dir_name+'2/'+'N'+pad_int_zeros(img_neg_index, 6)+'.png', new_img_2)\n",
    "                    img_neg_index += 1\n",
    "            \n",
    "                \n",
    "        \n",
    "    def generate_train_val_test_datasets(self, test_rate=0.1, val_rate=0.2, randomize_test=False):\n",
    "        assert len(self.images_1) == len(self.images_2)\n",
    "        assert len(self.images_1) == len(self.cdmaps)\n",
    "        \n",
    "        len_dataset = len(self.images_1)\n",
    "        len_test_set = int(test_rate*len_dataset)\n",
    "        \n",
    "        if randomize_test:\n",
    "            test_indicies = random.sample(range(len(self.images_1)), len_test_set)\n",
    "        else:\n",
    "            test_indicies = range(len(self.images_1))[-len_test_set:]\n",
    "            \n",
    "        train_val_indicies = []\n",
    "        \n",
    "        for i in range(len_dataset):\n",
    "            if i not in test_indicies:\n",
    "                train_val_indicies.append(i)\n",
    "        \n",
    "        self.remove_and_create_dirs()\n",
    "        \n",
    "        self.create_patches(TRAIN_VAL_TEST_DIR+'test/', test_indicies)\n",
    "        self.create_patches(TRAIN_VAL_TEST_DIR+'train/', train_val_indicies)\n",
    "        \n",
    "        train_val_patches = os.listdir(TRAIN_VAL_TEST_DIR+'train/1/')\n",
    "        val_indicies = random.sample(range(len(train_val_patches)), int(val_rate*len(train_val_patches)))\n",
    "        val_patches = []\n",
    "        for i in val_indicies:\n",
    "            val_patches.append(train_val_patches[i])\n",
    "        \n",
    "        for patch in val_patches:\n",
    "            os.rename(TRAIN_VAL_TEST_DIR+'train/1/'+patch, TRAIN_VAL_TEST_DIR+'val/1/'+patch)\n",
    "            os.rename(TRAIN_VAL_TEST_DIR+'train/2/'+patch, TRAIN_VAL_TEST_DIR+'val/2/'+patch)\n",
    "            os.rename(TRAIN_VAL_TEST_DIR+'train/gt/'+patch, TRAIN_VAL_TEST_DIR+'val/gt/'+patch)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_1 = []\n",
    "images_2 = []\n",
    "gt_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path preparation\n",
    "if DATASET == 'AICD':\n",
    "    INVERT_GT = False\n",
    "    for i_scene in range(100):\n",
    "        for i_view in range(5):\n",
    "            img_dir = DATASET_DIR+'Images_NoShadow/'\n",
    "            gt_dir = DATASET_DIR+'GroundTruth/'\n",
    "            base_name = 'Scene'+pad_int_zeros(i_scene, 4)+'_View'+pad_int_zeros(i_view, 2)\n",
    "\n",
    "            img_1 = img_dir+base_name+'_moving.png'\n",
    "            img_2 = img_dir+base_name+'_target.png'\n",
    "            gt = gt_dir+base_name+'_gtmask.png'\n",
    "            if not((os.path.isfile(img_1)) and (os.path.isfile(img_2)) and (os.path.isfile(gt))):\n",
    "                print(base_name+' not exists')\n",
    "            else:\n",
    "                images_1.append(img_1)\n",
    "                images_2.append(img_2)\n",
    "                gt_array.append(gt)\n",
    "elif DATASET == 'TSUNAMI':\n",
    "    INVERT_GT = True\n",
    "    for i_image in range(100):\n",
    "        base_name = pad_int_zeros(i_image, 8)\n",
    "\n",
    "        img_1 = DATASET_DIR+'t0/'+base_name+'.jpg'\n",
    "        img_2 = DATASET_DIR+'t1/'+base_name+'.jpg'\n",
    "        gt = DATASET_DIR+'ground_truth/'+base_name+'.bmp'\n",
    "        if not((os.path.isfile(img_1)) and (os.path.isfile(img_2)) and (os.path.isfile(gt))):\n",
    "            print(img_1, img_2, gt, ' not exists')\n",
    "        else:\n",
    "            images_1.append(img_1)\n",
    "            images_2.append(img_2)\n",
    "            gt_array.append(gt)\n",
    "else:\n",
    "    raise ValueError('Dataset must be one of [\\'AICD\\', \\'TSUNAMI\\']')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:23<00:00,  2.86s/it]\n",
      "100%|██████████| 450/450 [21:59<00:00,  2.93s/it]\n"
     ]
    }
   ],
   "source": [
    "dp = DataPreparator(images_1, images_2, gt_array, 240, 192, invert_gt=INVERT_GT)\n",
    "dp.remove_and_create_dirs()\n",
    "dp.generate_train_val_test_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
